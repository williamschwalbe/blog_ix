---
title: My first kaggle comp
author: Will
date: '2018-06-12'
slug: my-first-kaggle-comp
categories: []
tags: []
---
##Introduction
This is my first kaggle competition and the goal is to use the given data about people applying for loans to build a model to accurately predict whether or not the person will defualt on their loan.
##Loading libraries

Step one was loading up all the necesary libariries
```{r loading libraries, eval=FALSE}
library(ISLR)
library(ROCR)
library(Epi)
library(vcdExtra)
library(MASS)
library(ggplot2)
library(dplyr)
library(mlbench)
library(vcdExtra)
library(ISLR)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(Metrics)
library(caret)
```

#Pre-Processing
I then loaded in the necesary train and test files and filled in the na's in the data with the appropraite median for that column
```{r, eval=FALSE} 
train_data <- read.csv("CopyOfapplication_train.csv", stringsAsFactors = T)
head(test_data) <- read.csv("CopyOfapplication_test.csv", stringsAsFactors = T)
test_id <-test_data$SK_ID_CURR
preProcValues <- preProcess(train_data, method = c("center", "scale", "medianImpute"))
train_data <- predict(preProcValues, train_data)
preProcValues1 <- preProcess(test_data, method = c("center", "scale", "medianImpute"))
test_data <- predict(preProcValues1, test_data)
```

I then removed all columns with near zero variance from one another
```{r, eval=FALSE}

train_data.nzv <- nearZeroVar(train_data, saveMetrics = TRUE)
train_data <- train_data[, !train_data.nzv$nzv]
```
I then removed highly correlated features
```{r, eval=FALSE}
train_data.correlated <- findCorrelation(cor(train_data,use ="pairwise.complete.obs"), cutoff = 0.65, verbose = TRUE, exact = TRUE)
train_data <- train_data[, -train_data.correlated]
```
creating a subtrain and test set to test the model further on
```{r, eval=FALSE}
train <- sample(c(T, F), nrow(train_data), replace = TRUE, prob = c(0.8, 0.2))

train_data.train <- train_data[train,]
train_data.test <- train_data[!train,]
```
I then created a glm of all remaining inputs relative to the target value-whether or not someone in the group defaulted- and found the statistically most important variables
```{r, eval=FALSE}
fit_glm <- train(TARGET ~ ., data = train_data.train, method = "glm",
                  #metric =,
                  # na.action = na.pass,
                  trControl = trainControl(
                    method = "cv",
                    number = 10,
                    classProbs = TRUE,
                    verboseIter = TRUE
                  ))
```
```{r, eval=FALSE}
summary(glm)
```
## Model Creation
##Neural Net
Then using the most statistically important variables i created a neural network that was cross validataed with 8 folds
```{r,eval=FALSE}
fit_nnet <- train(TARGET ~ FLAG_OWN_CAR+AMT_INCOME_TOTAL+AMT_GOODS_PRICE +AMT_ANNUITY + AMT_CREDIT++REGION_POPULATION_RELATIVE +DAYS_BIRTH  +DAYS_REGISTRATION+DAYS_ID_PUBLISH +FLAG_WORK_PHONE  +REGION_RATING_CLIENT_W_CITY   +REG_CITY_NOT_LIVE_CITY +EXT_SOURCE_2 +FLAG_DOCUMENT_3+FLAG_DOCUMENT_6+DAYS_LAST_PHONE_CHANGE+AMT_REQ_CREDIT_BUREAU_YEAR , data = train_data.train, method = "nnet",
                  #metric =,
                  na.action = na.pass,
                  trControl = trainControl(
                    method = "cv",
                    number = 8,
                    #classProbs = TRUE,
                    verboseIter = TRUE
                  ))
```
## Decision Tree
I the used the same statistics to create a decision tree as this is a classification question so i this to be a fit model for the task at hand
```{r, eval=FALSE}
fit_tree <- train(TARGET ~ FLAG_OWN_CAR+AMT_INCOME_TOTAL+AMT_GOODS_PRICE +AMT_ANNUITY + AMT_CREDIT+REGION_POPULATION_RELATIVE +DAYS_BIRTH  +DAYS_REGISTRATION+DAYS_ID_PUBLISH +FLAG_WORK_PHONE  +REGION_RATING_CLIENT_W_CITY   +REG_CITY_NOT_LIVE_CITY +EXT_SOURCE_2 +FLAG_DOCUMENT_3+FLAG_DOCUMENT_6+DAYS_LAST_PHONE_CHANGE+AMT_REQ_CREDIT_BUREAU_YEAR ,data = train_data.train, method = "rpart",
                  na.action = na.pass,
                  trControl = trainControl(
                    method = "cv",
                    number = 10,
                    # classProbs = TRUE,
                    # summaryFunction = twoClassSummary,
                    verboseIter = TRUE
                  ))
```
## Creating Predictions
I then carried out the predictions according to each model
```{r, eval=FALSE}
prediction_tree <- predict(fit_tree,test_data)
prediction_nnet <- predict(fit_nnet,test_data)
```
I then combined the two models predicitons by taking the average of the two and rounding it to either a 0 or 1
```{r, eval=FALSE}
credit.df <- data.frame(SK_ID_CURR=test_data$SK_ID_CURR, TARGET = (prediction_nnet+prediction_tree)/2 )
```
Finally i created a new file with a data frame containing the id's of the test set and the comibined predictions from the two models and converted it to a csv file ready for submission as accordingl
```{r, eval=FALSE}
write.csv(credit.df, "kaggle_submission.csv", row.names = FALSE, quote = FALSE)
head(test_data)
